import pandas as pd
import os
import torch
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm

# --- 1. å…¨å±€ FinBERT æ¨¡å‹å’Œè®¾å¤‡è®¾ç½® ---
FINBERT_MODEL_NAME = '/share/liuyuqing/causal_net/data/finbert_hpc_files'

# å‡è®¾ DEVICE å˜é‡å·²åœ¨å…¶ä»–åœ°æ–¹å®šä¹‰ï¼Œè¿™é‡Œä¸ºäº†å®Œæ•´æ€§æ·»åŠ ä¸€ä¸ªå®šä¹‰
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {DEVICE}")

try:
    # å¼ºåˆ¶æœ¬åœ°æ–‡ä»¶åŠ è½½
    tokenizer = AutoTokenizer.from_pretrained(FINBERT_MODEL_NAME, local_files_only=True)
    model = AutoModel.from_pretrained(FINBERT_MODEL_NAME, local_files_only=True).to(DEVICE)
    model.eval()
    print("âœ… FinBERT æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸï¼ˆæœ¬åœ°æ¨¡å¼ï¼‰ã€‚")
except Exception as e:
    print(f"è‡´å‘½é”™è¯¯ï¼šFinBERT æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    raise

# --- 2. ç¨³å¥çš„æ•°æ®åŠ è½½å‡½æ•°ï¼ˆä¿®æ­£åˆ†éš”ç¬¦å’Œåˆ—åï¼‰ ---

# âš ï¸ ä¿®æ­£åˆ—åä»¥åŒ¹é… Tab åˆ†éš”åçš„å­—æ®µé¡ºåºå’Œæ•°é‡ (7ä¸ªå­—æ®µ)
# å®é™…æ•°æ®é¡ºåº: date, datetime_col, stock_ticker, company_name, title, summary, link
COLUMN_NAMES = ['date', 'datetime_col', 'stock_ticker', 'company_name', 'title', 'summary', 'link']
NUM_COLUMNS = len(COLUMN_NAMES)
SEPARATORS_TO_TRY = ['\t', '|', ';', ',']  # ç¡®ä¿ \t ä¼˜å…ˆ


def load_all_event_files(data_dir: str) -> pd.DataFrame:
    """
    éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰äº‹ä»¶ CSV æ–‡ä»¶ï¼Œå¹¶å¼ºåˆ¶å¤„ç† Tab åˆ†éš”ç¬¦å’Œç¼ºå¤±çš„å¤´è¡Œã€‚
    """
    all_files = []

    print(f"Scanning directory: {data_dir}")

    for root, _, files in os.walk(data_dir):
        for file_name in tqdm(files, desc="Loading raw files"):
            if file_name.endswith('.csv'):
                file_path = os.path.join(root, file_name)
                df = None

                for sep in SEPARATORS_TO_TRY:
                    try:
                        # å¼ºåˆ¶ä½¿ç”¨ header=None, names=COLUMN_NAMES
                        df = pd.read_csv(
                            file_path,
                            sep=sep,
                            engine='python',
                            header=None,
                            names=COLUMN_NAMES,
                            on_bad_lines='skip'
                        )

                        # æ£€æŸ¥åˆ—æ•°æ˜¯å¦åŒ¹é… (7åˆ—) ä¸”æ•°æ®ä¸ä¸ºç©º (è¿™æ˜¯æˆåŠŸè¯»å–çš„æ ‡å¿—)
                        if df.shape[1] == NUM_COLUMNS and not df.empty:
                            # print(f"  --> Successfully read {file_name} with separator: '{sep}'")
                            break
                        else:
                            df = None
                            continue

                    except Exception:
                        df = None
                        continue

                if df is not None and not df.empty:
                    all_files.append(df)
                # else:
                # print(f"Warning: Could not read file {file_name}")

    if not all_files:
        print("Error: No files were loaded successfully.")
        return pd.DataFrame(columns=COLUMN_NAMES)

    final_raw_df = pd.concat(all_files, ignore_index=True)

    # ğŸ“¢ å…³é”®æ•°æ®æ¸…ç†å’Œå¯¹é½å‡†å¤‡
    if not final_raw_df.empty:
        # 1. Ticker è§„èŒƒåŒ–ï¼šä½¿ç”¨æ­£ç¡®çš„ 'stock_ticker' åˆ—ï¼Œå¹¶è½¬ä¸ºå¤§å†™
        final_raw_df['ticker'] = final_raw_df['stock_ticker'].astype(str).str.upper()

        # 2. æ—¥æœŸè§„èŒƒåŒ–ï¼šç¬¬ 1 åˆ— (date) æœ¬èº«å·²ç»æ˜¯å¹²å‡€çš„æ—¥æœŸï¼Œç¡®ä¿å®ƒæ˜¯å­—ç¬¦ä¸²
        final_raw_df['date'] = final_raw_df['date'].astype(str)

        # 3. ä¸¢å¼ƒä¸éœ€è¦çš„åˆ—å¹¶è¿›è¡Œç®€å•æ¸…æ´—
        final_raw_df = final_raw_df.drop(columns=['datetime_col', 'company_name', 'stock_ticker', 'link'],
                                         errors='ignore')
        final_raw_df = final_raw_df.dropna(subset=['date', 'ticker', 'title', 'summary'])

        print("DEBUG: äº‹ä»¶æ•°æ® Ticker å’Œ Date æ ¼å¼å·²æ¸…ç†ã€‚")

    print(f"\nSuccessfully loaded and merged {len(all_files)} files.")
    print(f"Total rows in raw event data: {len(final_raw_df)}")

    return final_raw_df


# --- 3. æ–°é—»æ–‡æœ¬èšåˆå‡½æ•° (ä¿æŒä¸å˜) ---

def aggregate_news_text(raw_data_df: pd.DataFrame) -> pd.DataFrame:
    """
    æŒ‰ 'date' å’Œ 'ticker' èšåˆæ–°é—»æ–‡æœ¬ã€‚
    """
    print("\n--- æ­¥éª¤ 1/2: èšåˆäº‹ä»¶æ–‡æœ¬ ---")

    # æ³¨æ„ï¼šç°åœ¨ raw_data_df ä¸­åŒ…å«äº†æ­£ç¡®çš„ 'date' å’Œ 'ticker' åˆ—
    raw_data_df['full_text'] = raw_data_df['title'].astype(str) + ' [SEP] ' + raw_data_df['summary'].fillna('').astype(
        str)

    # æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œå¹¶å°†æ‰€æœ‰ full_text è¿æ¥èµ·æ¥
    aggregated_df = raw_data_df.groupby(['date', 'ticker']).agg(
        aggregated_text=('full_text', lambda x: ' '.join(x.astype(str)))
    ).reset_index()

    # é¿å…æé•¿æ–‡æœ¬ (BERTé™åˆ¶512 tokensï¼Œè¿™é‡Œç²—ç•¥åœ°é™åˆ¶å­—ç¬¦æ•°)
    MAX_CHAR_LENGTH = 1000
    aggregated_df['aggregated_text'] = aggregated_df['aggregated_text'].str.slice(0, MAX_CHAR_LENGTH)

    print(f"èšåˆå®Œæˆã€‚å¾—åˆ° {len(aggregated_df)} ä¸ª (æ—¥æœŸ, è‚¡ç¥¨) äº‹ä»¶ã€‚")
    return aggregated_df


# --- 4. FinBERT ç¼–ç å‡½æ•° (ä¿æŒä¸å˜) ---

def encode_texts_to_embeddings(texts: pd.Series, batch_size: int = 64) -> list:
    """
    ä½¿ç”¨ FinBERT æ¨¡å‹æ‰¹é‡ç¼–ç æ–‡æœ¬ï¼Œå¹¶è¿”å› [CLS] Token çš„éšè—çŠ¶æ€ä½œä¸ºåµŒå…¥å‘é‡ã€‚
    """
    embeddings = []

    for i in tqdm(range(0, len(texts), batch_size), desc="--- æ­¥éª¤ 2/2: FinBERT ç¼–ç ä¸­"):
        batch_texts = texts.iloc[i:i + batch_size].tolist()

        try:
            inputs = tokenizer(
                batch_texts,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            ).to(DEVICE)

            with torch.no_grad():
                outputs = model(**inputs)

            # ç­–ç•¥ï¼šæå– [CLS] Token çš„éšè—çŠ¶æ€ä½œä¸ºæ–‡æœ¬åµŒå…¥ (ç´¢å¼• 0)
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.extend(batch_embeddings)

        except Exception as e:
            # æ‰¹å¤„ç†å¤±è´¥å®¹é”™å¤„ç†
            print(f"\n[Warning] Error in batch {i // batch_size}: {e}. Filling with zero vectors.")
            num_failed = len(batch_texts)
            zero_embedding = torch.zeros(model.config.hidden_size).cpu().numpy()
            embeddings.extend([zero_embedding] * num_failed)

    return embeddings


# --- 5. æµç¨‹æ§åˆ¶ä¸»å‡½æ•° (ä¿æŒä¸å˜) ---

def process_full_event_data(raw_data_df: pd.DataFrame, batch_size: int = 64) -> pd.DataFrame:
    """
    FinBERT å¤„ç†çš„æµç¨‹æ€»æ§ï¼šèšåˆ -> ç¼–ç  -> ç»“æœåˆå¹¶ã€‚
    """
    if raw_data_df.empty:
        print("è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ FinBERT å¤„ç†ã€‚")
        return pd.DataFrame()

    # 1. æ–‡æœ¬èšåˆ
    aggregated_df = aggregate_news_text(raw_data_df)

    # 2. æ‰¹é‡ç¼–ç 
    text_series = aggregated_df['aggregated_text']
    embeddings = encode_texts_to_embeddings(text_series, batch_size=batch_size)

    # 3. å°†åµŒå…¥å‘é‡æ·»åŠ åˆ° DataFrame
    aggregated_df['event_embedding'] = embeddings

    # 4. æ¸…ç†ä¸­é—´æ–‡æœ¬åˆ—
    final_embedded_df = aggregated_df.drop(columns=['aggregated_text'])

    print(f"\nå¤„ç†å®Œæˆã€‚åµŒå…¥å‘é‡å·²ç”Ÿæˆã€‚")
    return final_embedded_df


# --- 6. ä¸»æ‰§è¡Œå— (ä¿æŒä¸å˜) ---

if __name__ == '__main__':
    # âš ï¸ è¯·ç¡®ä¿è¿™ä¸ªè·¯å¾„æ˜¯æ­£ç¡®çš„ CMIN-US æ•°æ®é›†è·¯å¾„
    EVENT_DATA_PATH = "/share/liuyuqing/causal_net/data/CMIN-Dataset-main/CMIN-US/news/raw"
    OUTPUT_FILE = '/share/liuyuqing/causal_net/cmin_US_event_embeddings_processed.pkl'  # ç¡®ä¿è¾“å‡ºè·¯å¾„æ­£ç¡®

    print("--- 1. å¯åŠ¨åŸå§‹äº‹ä»¶æ•°æ®åŠ è½½ ---")
    raw_event_data_df = load_all_event_files(EVENT_DATA_PATH)

    # 2. è¿›è¡Œ FinBERT å¤„ç†
    if not raw_event_data_df.empty:
        print("\n--- 2. å¯åŠ¨ FinBERT ç¼–ç å’Œèšåˆ ---")

        final_embedded_df = process_full_event_data(raw_event_data_df, batch_size=64)

        # 3. æ‰“å°å’Œä¿å­˜ç»“æœ
        print("\n--- 3. FinBERT ç¼–ç å®Œæˆã€‚ ---")
        print(f"æœ€ç»ˆäº‹ä»¶è®°å½•æ•° (æŒ‰å¤©/è‚¡ç¥¨èšåˆå): {len(final_embedded_df)}")
        print("\nDataFrame å¤´éƒ¨:")
        print(final_embedded_df.head())

        if 'event_embedding' in final_embedded_df and not final_embedded_df.empty:
            final_embedded_df['date'] = final_embedded_df['date'].astype(str)

            final_embedded_df.to_pickle(OUTPUT_FILE)
            print(f"\n æˆåŠŸå°†åµŒå…¥ç»“æœä¿å­˜åˆ°: {OUTPUT_FILE}")
    else:
        print("æ— æ³•åŠ è½½åŸå§‹æ•°æ®ï¼ŒFinBERT ç¼–ç æµç¨‹ä¸­æ­¢ã€‚")